# some static filters in here.
# 1) the data from the UK for 3/15 ONLY has cases from channel islands
# So we just throw this point away, so the chart will at least connect over the point which will
# then be totally null.
# 2) then similarly, within the US on March 9th
#  a) - for Washington state, the data on 3/9/2020 ONLY exists for Grant County, and it has only one confirmed case, who died. So just removing this one event from the set, allows the chart to connect over for the whole state of washington bewteen 3/8/ and 3/10, instead of swooping all the way down to "0" for "Sick" patients.
# b) likewise for California, there are only data points for Sonoma, Placer, Shasta and Orange Counties, and many other counties that on 3/8 and 3/10 show significant counts, are just not there.
# Filtering them out, paradoxically allows the charting layer to connect the two points.
# likewise - on 3/23 there's a point for French Polynesia that is clearly just some egregious mistake
# because quick googling confirms there are not suddenly 16814 confirmed cases there.

[covid_19_all_data]
definition = `covid_19_csse_daily_reports` | `append_covid_tracking_com_data` | `append_lockdown_data`

#index=covid19 sourcetype="csse_covid_19_daily_report" | fields - punct host eventtype index linecount  splunk_server splunk_server_group timeendpos timestartpos _bkt _cd _eventtype_color _indextime _kv _raw _si _subsecond | outputlookup csse_covid19_daily_reports
[covid_19_csse_daily_reports]
definition = inputlookup merged_daily_reports | fillnull Confirmed Recovered Deaths value=0 | eval Sick=max(Confirmed,Recovered+Deaths) - Recovered - Deaths | search `standard_search_filters`

[append_covid_tracking_com_data]
definition = inputlookup covidtracking_com_states_daily_testing append=t | eval _time=if(isnull(_time), strptime(date, "%Y%m%d"),_time) | eval is_ct_com=if(isnotnull(hash),1,0) | eval Country_Region=if(is_ct_com==1, "US",Country_Region) | lookup us_states code as state OUTPUTNEW state as Province_State

[append_lockdown_data]
definition = append [| inputlookup netlify_com_lockdown_dates | search Level="State" OR Level="National" | rename "Start date" as Lockdown_Started Country as Country_Region Place as Province_State | eval Country_Region=if(Country_Region=="United States","US",Country_Region) | eval _time=strptime(Lockdown_Started, "%Y-%m-%d") | eval Lockdown_Started=1 | table _time Country_Region Province_State Lockdown_Started]


[standard_cleanup_and_normalization]
definition = bin _time span=1d | `extract_canadian_province_instead_of_city_name` | `get_country_statistics`| `extract_us_state_instead_of_county_name` | `fix_country_names` | `fix_province_and_state_names` | eval Province_State=coalesce(Province_State,Country_Region) | eval Province_State=if(Province_State="French Polynesia" AND Confirmed=19874,Country_Region,Province_State)


[standard_search_filters]
definition = NOT Country_Region="Others" | search NOT (Country_Region="United Kingdom" Province_State="Channel Islands" Last_Update="2020-03-15T18:20:19") NOT (Last_Update="2020-03-09*" Country_Region="US" ( Province_State="Grant County, WA" OR Province_State="* County, CA"))

# There are some inconsistent names, like "Iran" vs "Iran, Islamic Republic of".  We use a simple
# file-based lookup to map these secondary names back to the primary name and this is the
# macro that actually does that.
# also. there's one datapoint that lists Hong Kong with Country_Region=China, and all the others let
# HK be its own country.  Picking HK/HK as the winner here.
[fix_country_names]
definition = lookup normalized_country_names name as Country_Region OUTPUT normalized_name as Country_Region_Normalized | eval Country_Region=case(isnotnull(Country_Region_Normalized),Country_Region_Normalized,Country_Region=="China" AND Province_State=="Hong Kong","Hong Kong",true(),Country_Region)

[fix_province_and_state_names]
definition = lookup normalized_province_and_state_names name as Province_State OUTPUT normalized_name as Province_State_Normalized | eval Province_State=coalesce(Province_State_Normalized,Province_State) | eval Province_State=if((Country_Region=="Austria" OR Country_Region=="Iraq" OR Country_Region=="Lebanon") AND Province_State=="None",Country_Region,Province_State)

[get_country_statistics]
definition = lookup countries country as Country_Region OUTPUT population_2020 as population urban_percentage median_age density area | lookup us_states state as Province_State OUTPUT median_age

[set_time_from_filename]
definition =  rex field=source ".+[/|\\\\](?<filename_date>.+)\.csv$" | eval filename_epoch=strptime(filename_date, "%m-%d-%Y") | eval _time=if(isnotnull(filename_epoch),filename_epoch,_time)

# THIS IS THE OLD STRATEGY To deal with the old dupes. Only used in checklist.conf
[ignore_old_duplicate_reports]
definition =  rex field=source ".+[/|\\\\](?<filename_date>.+)\.csv$" | eval last_update=strptime(filename_date, "%m-%d-%Y") | bin _time span=1d | bin last_update span=1d | where sourcetype!="csse_covid_19_daily_report" OR last_update==_time


#DEPRECATED.   I have to replace this with something, but now that "100th case" is parametrized, this
# random 3 day shift isn't really ok anymore.
#
# WHAT THIS WAS --- China begins in the dataset at 547.  But the chart is supposed to show day 0
# as the day that it passed 100.  Assuming the prior days were lower than 547 but not below,
# extrapolating the line on the log-axis curve, this suggests that China's true "100 cases" day
# was 3 days prior.
[unshift_china]
definition = eval days_since_threshold_passed=if(Country_Region=="China",days_since_threshold_passed + 3,days_since_threshold_passed)


[get_count_of_days_since_lockdown_start]
definition = sort 0 +Country_Or_State +_time  | eval starting_time=if(Lockdown_Started==1,_time,null()) | eventstats min(starting_time) as starting_time by Country_Or_State | eval _secret_time=_time | eval _time=_time-starting_time | eval days_since_lockdown_started = round(_time/86400) | fields - starting_time | eval _time = _secret_time

# Figures out on what day they passed the nth case, and creates a field called
# "days_since_threshold_passed".   Other SPL downstream will use this new field.
[get_count_of_days_since_nth_case(3)]
args = n, splitBy, statistic
definition = sort 0 +$splitBy$ +_time  | eval past_$n$ = if($statistic$>$n$,1,0) | streamstats sum(past_$n$) as starting_point by $splitBy$ | eval starting_time_since_nth_case=if(starting_point==1,_time,null()) | eventstats min(starting_time_since_nth_case) as starting_time_since_nth_case by $splitBy$ | eval _secret_time=_time | eval _time=_time-starting_time_since_nth_case | eval days_since_threshold_passed = round(_time/86400) | fields - past_$n$ | eval _time = _secret_time

# parametrized version that uses a default of 'Confirmed' ie 'total cumulative cases to date' for the statistic.
[get_count_of_days_since_nth_case(2)]
args = n, splitBy
definition = `get_count_of_days_since_nth_case($n$, $splitBy$, Confirmed)`


# parametrized version that uses a default of 'Confirmed' ie 'total cumulative cases to date' for
# the statistic, and uses a default of Country_Region for the split-by field.
[get_count_of_days_since_nth_case(1)]
args = n
definition = `get_count_of_days_since_nth_case($n$, Country_Region)`


[get_country_slash_state_and_spl(1)]
args = breakApartStates
definition = eval Country_Or_State = if ("$breakApartStates$"=="no" OR Country_Region==Province_State OR Country_Region="France" OR Country_Region="United Kingdom",Country_Region,Country_Region + " - " + Province_State) | eval spl = if ("$breakApartStates$"=="no" OR Country_Region==Province_State,"Country_Region=\"" + Country_Region+"\"", "( Country_Region=\"" + Country_Region + "\" Province_State=\"" + Province_State + "\" )")


# at the beginning the US data is all reported by County and not by state.
# this just uses regexes to pullout the 2 letter state abbreviation
# this macro is really verbose and breaks things into very explicit steps because I wanted it to be
# as clear as possible (if tedious) to follow what I did here.
[extract_us_state_instead_of_county_name]
definition = rex field="Province_State" "(^(?<secret_county>.+) County)?,\s(?<secret_state>[A-Z]{2})\s?$" | eval County=if(isnull(County),secret_county,County) | rex field="Province_State" ",\s(?<secret_state_from_cruise_ship>[A-Z]{2})\s\(From .+ Princess\)$" | eval type=case(isnotnull(secret_state_from_cruise_ship), "from cruise ship",isnotnull(secret_state),"from county",true(),"actual state") | eval Province_State=case(isnotnull(secret_state_from_cruise_ship),secret_state_from_cruise_ship,isnotnull(secret_state),secret_state,true(),Province_State) | lookup us_states code as Province_State OUTPUT state as Repaired_Province_State | eval Province_State=if(isnotnull(Repaired_Province_State),Repaired_Province_State,Province_State)

# this needs to be replaced with something more clever with match() but this is faster for me to do right now.
[extract_canadian_province_instead_of_city_name]
definition =  eval Province_State=if(Country_Region=="Canada",case(Province_State LIKE "%, QC", "Quebec", Province_State LIKE "%, Alberta","Alberta", Province_State LIKE "%, ON","Ontario",true(),Province_State),Province_State)


# because the us data switched over from county-level to state-level,  it means if you get the list
# of all "Province_State" values you get a mishmash of counties and states.
# in some places in the app rather than using the macro to pull out hte state FROM the county name
# it's easier to just throw these away - notably for the US when rendering
# the pulldown of the 'real' states.
[filter_out_us_counties]
definition = search Country_Region!="US" OR (Province_State!="*, *" Province_State!="*Princess*")


# when you go blind you can blame splunk engineering. 10 years since macros.conf was created, it
# STILL wont allow the user to escape linebreaks. (like savedsearches.conf does).
# Note - there's a LOT of moving parts in here that we dont use. This SPL is copied over from our
# checklist.conf's in other apps.
[check_canary_version]
definition = rest /servicesNS/-/-/configs/conf-app splunk_server=local | where match(title,"dependency:app") or isnotnull(version) | rename "eai:acl.app" as app | eval appWithDependency=if(match(title,"dependency.app:*"),app,null()) | eval app=case(match(title,"dependency.app:*"),replace(title,"dependency.app:",""),isnotnull(version),app) | eventstats last(version) as hasVersion by app | append  [| rest /services/server/info splunk_server=local | rename serverName as instance_name | fields instance_name  ] | eventstats last(instance_name) as instance_name | search requiredVersion=* hasVersion=* | fields instance_name appWithDependency app requiredVersion hasVersion | eval has=split(hasVersion,".")  \
| eval req=split(requiredVersion,".") | eval req_weighted = 1000000000 * tonumber(mvindex(req,0)) + if(mvcount(req)>1,1000000 * tonumber(mvindex(req,1)),0) + if(mvcount(req)>2,1000 * tonumber(mvindex(req,2)),0)  + if(mvcount(req)>3,1 * tonumber(mvindex(req,3)),0) | eval has_weighted = 1000000000 * tonumber(mvindex(has,0)) + if(mvcount(has)>1,1000000 * tonumber(mvindex(has,1)),0) + if(mvcount(has)>2,1000 * tonumber(mvindex(has,2)),0)  + if(mvcount(has)>3,1 * tonumber(mvindex(has,3)),0) | eval metric=if(has_weighted<req_weighted, \
    "ERROR - the " . appWithDependency + " app needs at least version " + requiredVersion + " of " + app + " installed.", null()) | eval severity_level=if(has_weighted<req_weighted,3,0) | fields instance_name metric severity_level appWithDependency app requiredVersion hasVersion

# this is a weird thing I was working on,  that you stick in before the chart command.
# it tries to add up all the province_state values for each country, sum up all their states
# and insert a whole new row for the "overall country".  It has issues and I have abandoned it for
# now to just get the core flattening of state + country going.
#| append [
#  | makeresults | fields - _time
#  | eval Country_Region=split("Australia,Canada,China,US", ",")
#  | mvexpand Country_Region
#  | eval spl="Country_Region=\"" + Country_Region + "\""
#  | eval Country_Or_State = Country_Region
#  | eval is_rollup=1
#]
#| eventstats values(_time) as times by Country_Region
#| eval times=if(isnull(is_rollup),_time,times)
#| mvexpand times
#| rename times as _time
#| eventstats sum(*) as rollup_* by Country_Region _time
#| eval Confirmed=coalesce(Confirmed,rollup_Confirmed)
#| eval Sick=coalesce(Sick,rollup_Sick)
#| eval Recovered=coalesce(Recovered,rollup_Recovered)
#| eval Deaths=coalesce(Deaths,rollup_Deaths)
#| fields - rollup_* all_times
#| search $Country_Or_State$
